{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVzfmpAAiqPy",
        "outputId": "4fd196ea-f7e1-43ae-8236-911f05c89564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cs378_fp'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 67 (delta 14), reused 0 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects: 100% (67/67), 38.61 KiB | 564.00 KiB/s, done.\n",
            "/content/cs378_fp\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/johnjin0000/cs378_fp.git\n",
        "%cd cs378_fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBnvMLQPiy_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42043313-9093-4522-a19b-8b1fdf2cdc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adversarial_set.json  helpers.py  requirements.txt  test_contrast.json\n",
            "contrast.csv\t      README.md   run.py\t    test_original.json\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.0.0\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.4.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.2->-r requirements.txt (line 1)) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.0.0->-r requirements.txt (line 2)) (1.5.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.0.0->-r requirements.txt (line 2)) (2023.4.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.0.0->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.4.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.0.0->-r requirements.txt (line 2)) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.0.0->-r requirements.txt (line 2)) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.0.0->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.0.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 1)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 1)) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=cd072f38411fd432c42a41b7c21edb9158a7682410ef6750996d54f79c00a9fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, xxhash, torch, sacremoses, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.0.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.4.0 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 sacremoses-0.0.53 tokenizers-0.13.3 torch-1.11.0 transformers-4.16.2 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install textattack[tensorflow]"
      ],
      "metadata": {
        "id": "FRnSX5Ob1XAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1fc6e2-858f-4a7a-8b5d-26d74c079708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textattack[tensorflow]\n",
            "  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.11.0)\n",
            "Collecting click<8.1.0\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.6.2)\n",
            "Collecting transformers>=4.21.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (4.65.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (3.8.1)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.22.4)\n",
            "Collecting pycld2\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.7.1)\n",
            "Collecting OpenHowNet\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.10.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (3.12.0)\n",
            "Collecting pinyin==0.4.0\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.42.1)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting datasets==2.4.0\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lemminflect\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (9.1.0)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flair\n",
            "  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict\n",
            "  Downloading lru_dict-1.1.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Collecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.13.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==2.9.0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (3.2.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (23.1)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (3.8.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack[tensorflow]) (2023.4.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (0.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (3.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (1.6.3)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (16.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (1.54.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (0.2.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2.8.2)\n",
            "Collecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack[tensorflow]) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack[tensorflow]) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack[tensorflow]) (6.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (4.9.2)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (4.3.1)\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (0.2.7)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting pytorch-revgrad\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.1\n",
            "  Downloading transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.121-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gdown==4.4.0\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (1.2.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack[tensorflow]) (4.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack[tensorflow]) (1.2.0)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1->textattack[tensorflow]) (0.40.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack[tensorflow]) (1.3.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair->textattack[tensorflow]) (6.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[tensorflow]) (0.18.3)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[tensorflow]) (0.10.9.7)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[tensorflow]) (3.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[tensorflow]) (2.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack[tensorflow]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack[tensorflow]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack[tensorflow]) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair->textattack[tensorflow]) (3.1.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (2.3.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (3.4.3)\n",
            "Requirement already satisfied: transformers[torch]<5.0,>=4.1 in /usr/local/lib/python3.10/dist-packages (from transformer-smaller-training-vocab>=0.2.1->flair->textattack[tensorflow]) (4.16.2)\n",
            "Collecting botocore<1.30.0,>=1.29.121\n",
            "  Downloading botocore-1.29.121-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair->textattack[tensorflow]) (0.2.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (2.1.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack[tensorflow]) (2.4.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[tensorflow]) (3.2.2)\n",
            "Building wheels for collected packages: pinyin, gdown, mpld3, pycld2, word2number, docopt, sqlitedict, langdetect, pptree\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630492 sha256=eb8ccf101a89c6b11112743bcceb3d4a69210e62a22c8af18c8cb8300f4bb9a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14776 sha256=0cb5e0861371fd692e101aa9adb61877a5237cea8852df1b5728b0f50795b1fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=254f0480c487b10c8e06240bb3a918a0fbe1ecd3b93d8b729aaa7e520fcda323\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9915753 sha256=c39df44f9a542fb25489e79a6c4b5df974c70f1598a4195de6c5805f1da2a0af\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=56df910dae57f8f4aa03657123f5cd5b5a341258569e22d0e66aaff8478e006c\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=7ecf8c8766f85a93ce17319b9c54c305de16d335249d5846fb80c89d5384f902\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=3285b7e475ae8b3fec8515ffad76233e5d847de967399d4694b17af5ade2456e\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=6c5f62544162f07bcf444d12ab7a0a766ef12b7a210e8051827fe07bd140f8ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=ee73d35ff3c9c392ca00aaadd1958ff82e94658067bb0b48c22ee4567124f002\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "Successfully built pinyin gdown mpld3 pycld2 word2number docopt sqlitedict langdetect pptree\n",
            "Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, mpld3, lru-dict, keras, janome, flatbuffers, docopt, terminaltables, tensorflow-estimator, tensorboard-data-server, segtok, protobuf, num2words, lemminflect, langdetect, keras-preprocessing, jmespath, ftfy, dill, deprecated, conllu, click, anytree, wikipedia-api, tensorboardX, pytorch-revgrad, OpenHowNet, multiprocess, language-tool-python, huggingface-hub, botocore, transformers, s3transfer, google-auth-oauthlib, gdown, bpemb, tensorboard, datasets, boto3, bert-score, transformer-smaller-training-vocab, tensorflow, tensorflow-text, flair, textattack\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.14\n",
            "    Uninstalling multiprocess-0.70.14:\n",
            "      Successfully uninstalled multiprocess-0.70.14\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.4.0\n",
            "    Uninstalling huggingface-hub-0.4.0:\n",
            "      Successfully uninstalled huggingface-hub-0.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.16.2\n",
            "    Uninstalling transformers-4.16.2:\n",
            "      Successfully uninstalled transformers-4.16.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.2\n",
            "    Uninstalling tensorboard-2.12.2:\n",
            "      Successfully uninstalled tensorboard-2.12.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.0.0\n",
            "    Uninstalling datasets-2.0.0:\n",
            "      Successfully uninstalled datasets-2.0.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenHowNet-2.0 anytree-2.8.0 bert-score-0.3.13 boto3-1.26.121 botocore-1.29.121 bpemb-0.3.4 click-8.0.4 conllu-4.5.2 datasets-2.4.0 deprecated-1.2.13 dill-0.3.5.1 docopt-0.6.2 flair-0.12.2 flatbuffers-1.12 ftfy-6.1.1 gdown-4.4.0 google-auth-oauthlib-0.4.6 huggingface-hub-0.14.1 janome-0.4.2 jmespath-1.0.1 keras-2.9.0 keras-preprocessing-1.1.2 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.1.8 mpld3-0.3 multiprocess-0.70.13 num2words-0.5.12 pinyin-0.4.0 pptree-3.1 protobuf-3.19.6 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.6.0 segtok-1.5.11 sentencepiece-0.1.98 sqlitedict-2.1.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboardX-2.6 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0 terminaltables-3.1.10 textattack-0.3.8 transformer-smaller-training-vocab-0.2.3 transformers-4.28.1 wikipedia-api-0.5.8 word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8TGBnoVizqJ"
      },
      "outputs": [],
      "source": [
        "# train model on snli dataset\n",
        "!python3 run.py --do_train --task nli --dataset adversarial_set.json --model ./trained_model/ --output_dir ./trained_model/ --per_device_train_batch_size 8 --num_train_epochs 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on snli dataset, acc is 0.8917902708053589\n",
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "id": "frLBXXNO8_6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from textattack.models.wrappers.huggingface_model_wrapper import HuggingFaceModelWrapper\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack import Attacker, AttackArgs\n",
        "from textattack.attack_results import SuccessfulAttackResult\n",
        "import csv\n",
        "\n",
        "dataset = HuggingFaceDataset(\"snli\", None, \"train\", shuffle=True)\n",
        "dataset.filter_by_labels_((0, 1, 2))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./trained_model/\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./trained_model\", use_fast=True)\n",
        "attack = TextFoolerJin2019.build(HuggingFaceModelWrapper(model, tokenizer))\n",
        "\n",
        "attack_args = AttackArgs(\n",
        "    num_examples = 150,\n",
        "    disable_stdout=True,\n",
        ")\n",
        "\n",
        "attacker = Attacker(attack, dataset, attack_args)\n",
        "results = attacker.attack_dataset()\n",
        "\n",
        "# Open a CSV file in write mode\n",
        "with open('attack_dataset.csv', 'w', newline='') as csvfile:\n",
        "  # Create a CSV writer object\n",
        "  writer = csv.writer(csvfile)\n",
        "  write_lines = []\n",
        "  for result in results:\n",
        "    if isinstance(result, SuccessfulAttackResult):\n",
        "      label = result.original_result.ground_truth_output\n",
        "      premise, hypothesis = (result.original_result.attacked_text.text).split(\"\\n\")\n",
        "      write_lines.append([premise, hypothesis, label])\n",
        "\n",
        "  writer.writerow([\"premise\", \"hypothesis\", \"label\"])\n",
        "  # Write data to CSV file line by line\n",
        "  for line in write_lines:\n",
        "    writer.writerow(line)\n"
      ],
      "metadata": {
        "id": "Z7_lO90wt5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGBl0yzfi9kd"
      },
      "outputs": [],
      "source": [
        "# evaluate model on contrast set, 'eval_loss': 0.8511903285980225, 'eval_accuracy': 0.75\n",
        "!python3 run.py --do_eval --task nli --dataset test_contrast.json --model ./trained_model/ --output_dir ./eval_output/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on original set, 'eval_loss': 0.40389466285705566, 'eval_accuracy': 0.8700000047683716,\n",
        "!python3 run.py --do_eval --task nli --dataset test_original.json --model ./trained_model/after_training/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "id": "HosjLijGpPVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_nli(examples, tokenizer, max_seq_length=None):\n",
        "    max_seq_length = tokenizer.model_max_length if max_seq_length is None else max_seq_length\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples['premise'],\n",
        "        examples['hypothesis'],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    tokenized_examples['label'] = examples['label']\n",
        "    return tokenized_examples\n",
        "\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import datasets\n",
        "\n",
        "# Load model from checkpoint\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./trained_model/after_training/\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./trained_model/after_training/\", use_fast=True)\n",
        "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test original dataset\n",
        "orig_dataset = datasets.load_dataset('json', data_files=\"test_original.json\", field=\"data\")\n",
        "orig_eval_dataset = orig_dataset[\"train\"]\n",
        "orig_eval_dataset_featurized = orig_eval_dataset.map(\n",
        "  lambda exs: prepare_dataset_nli(exs, tokenizer, 128),\n",
        "  batched=True,\n",
        "  num_proc=2,\n",
        "  remove_columns=orig_eval_dataset.column_names\n",
        ")\n",
        "orig_pred, orig_labels, metrics = trainer.predict(orig_eval_dataset_featurized)\n",
        "\n",
        "# Test contrast dataset\n",
        "cont_dataset = datasets.load_dataset('json', data_files=\"test_contrast.json\", field=\"data\")\n",
        "cont_eval_dataset = cont_dataset[\"train\"]\n",
        "cont_eval_dataset_featurized = cont_eval_dataset.map(\n",
        "  lambda exs: prepare_dataset_nli(exs, tokenizer, 128),\n",
        "  batched=True,\n",
        "  num_proc=2,\n",
        "  remove_columns=cont_eval_dataset.column_names\n",
        ")\n",
        "cont_pred, cont_labels, metrics = trainer.predict(cont_eval_dataset_featurized)\n",
        "\n",
        "# Test Old Model before adversarial training\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./trained_model/\", num_labels=3)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./trained_model/\", use_fast=True)\n",
        "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
        "\n",
        "old_cont_pred, old_cont_labels, old_metrics = trainer.predict(cont_eval_dataset_featurized)"
      ],
      "metadata": {
        "id": "nB5aXE8NpipS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cont_pred_argmax = np.argmax(cont_pred, axis=1)\n",
        "print(cont_pred_argmax)\n",
        "print(cont_labels)\n",
        "cm = confusion_matrix(cont_pred_argmax, cont_labels)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLE8xtdGKS4u",
        "outputId": "ffbbd4af-b1a9-470a-96ee-fdc906fa67d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 0 2 2 2 2 0 2 2 2 1 2 1 2 0 2 1 2 2 2 1 2 2 2 2 1 2 1 2 1 1 0 2 2 2 2\n",
            " 1 1 2 2 0 2 0 2 2 2 2 2 1 1 2 1 1 2 2 1 2 2 2 2 1 2 1 0 0 0 2 2 2 2 2 2 0\n",
            " 2 2 1 2 2 0 2 1 1 1 2 1 0 0 0 0 0 1 0 0 2 2 1 0 1 1]\n",
            "[2 2 1 2 2 2 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 1 2 2 2 2 1 2 1 2 1 1 2 2 2 2 2\n",
            " 1 1 2 2 0 2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 2 2 2 2 2 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1]\n",
            "[[11  6  2]\n",
            " [ 2 18  6]\n",
            " [ 3  6 46]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "old_cont_pred_argmax = np.argmax(old_cont_pred, axis=1)\n",
        "print(old_cont_pred_argmax)\n",
        "print(old_cont_labels)\n",
        "cm2 = confusion_matrix(old_cont_pred_argmax, old_cont_labels)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74z8zGtprzh",
        "outputId": "08c44c40-1d7c-491f-a959-2b62b0a94aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 0 2 2 2 0 0 2 2 2 1 2 1 2 0 2 1 2 2 2 1 2 2 2 2 1 2 1 2 1 0 0 0 2 0 2\n",
            " 1 1 2 2 0 2 0 2 1 2 2 2 1 1 2 1 1 2 2 1 2 1 1 2 1 2 1 0 0 0 1 2 2 1 2 2 0\n",
            " 2 2 1 2 1 0 2 1 1 1 2 1 0 0 0 0 0 1 0 0 2 2 1 0 1 1]\n",
            "[2 2 1 2 2 2 2 1 2 1 2 1 2 2 2 2 2 1 2 2 2 1 2 2 2 2 1 2 1 2 1 1 2 2 2 2 2\n",
            " 1 1 2 2 0 2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 2 2 2 2 2 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1]\n",
            "[[11  7  5]\n",
            " [ 3 17 11]\n",
            " [ 2  6 38]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "both_correct = 0\n",
        "old_correct = 0\n",
        "new_correct = 0\n",
        "both_wrong = 0\n",
        "\n",
        "for i in range(len(cont_labels)):\n",
        "  if old_cont_pred_argmax[i] == cont_labels[i] and cont_pred_argmax[i] == cont_labels[i]:\n",
        "    both_correct += 1\n",
        "  elif old_cont_pred_argmax[i] == cont_labels[i]:\n",
        "    old_correct += 1\n",
        "  elif cont_pred_argmax[i] == cont_labels[i]:\n",
        "    new_correct += 1\n",
        "  else:\n",
        "    both_wrong += 1\n",
        "\n",
        "print(both_correct, old_correct, new_correct, both_wrong)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRpZeaFzOD_e",
        "outputId": "cddde528-b45a-4794-855c-cd94761e550d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66 0 9 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_examples = []\n",
        "for i in range(len(cont_labels)):\n",
        "  if old_cont_pred_argmax[i] != cont_labels[i] and cont_pred_argmax[i] == cont_labels[i]:\n",
        "    selected_examples.append((i, old_cont_pred_argmax[i]))\n",
        "\n",
        "import json\n",
        "\n",
        "with open('test_contrast.json', 'r') as jsonfile:\n",
        "    # Write data as JSON\n",
        "    contrast_exs = json.load(jsonfile)\n",
        "\n",
        "for idx in selected_examples:\n",
        "  print(contrast_exs[\"data\"][idx[0]])\n",
        "  print(\"old predicted\", idx[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXcGt3NFTSrG",
        "outputId": "a015fc36-73f2-4d2a-9dad-2b86dda43d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'premise': 'two boys reading superhero books', 'hypothesis': 'three boys reading a piece of literature.', 'label': 2}\n",
            "old predicted 0\n",
            "{'premise': 'Two women walk down a sidewalk along a busy street in a downtown area.', 'hypothesis': 'The beutiful women were walking downtown', 'label': 1}\n",
            "old predicted 0\n",
            "{'premise': 'The girls walk down the street.', 'hypothesis': 'Girls walk down the alley.', 'label': 2}\n",
            "old predicted 0\n",
            "{'premise': \"A man of the cloth puts a black substance on a man's forehead.\", 'hypothesis': 'The man puts something on the other mans nose.', 'label': 2}\n",
            "old predicted 0\n",
            "{'premise': 'The tattooed basketball player is about to dunk the ball.', 'hypothesis': 'The player has a book that says mother.', 'label': 2}\n",
            "old predicted 1\n",
            "{'premise': 'A basketball player with green shoes is dunking the ball in the net while the arena crowd looks on.', 'hypothesis': 'A famous NBA player is playing street ball in front of no one.', 'label': 2}\n",
            "old predicted 1\n",
            "{'premise': 'A frowning old man in a military cap and a civilian suit stands amongst a crowd of people outdoors carrying signs and balloons.', 'hypothesis': 'An happy military veteran watches as people protest the war.', 'label': 2}\n",
            "old predicted 1\n",
            "{'premise': 'A man in a black shirt, in a commercial kitchen, holding up meat he took out of a bag.', 'hypothesis': 'A man in a black shirt, in a home kitchen, holding up the old meat he took out of a bag.', 'label': 2}\n",
            "old predicted 1\n",
            "{'premise': 'Two men prepare a fish at a dock.', 'hypothesis': 'Two men have just come in from hunting all day', 'label': 2}\n",
            "old predicted 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_examples = []\n",
        "for i in range(len(cont_labels)):\n",
        "  if old_cont_pred_argmax[i] != cont_labels[i] and cont_pred_argmax[i] != cont_labels[i]:\n",
        "    selected_examples.append((i, old_cont_pred_argmax[i], cont_pred_argmax[i]))\n",
        "\n",
        "import json\n",
        "\n",
        "with open('test_contrast.json', 'r') as jsonfile:\n",
        "    # Write data as JSON\n",
        "    contrast_exs = json.load(jsonfile)\n",
        "\n",
        "for idx in selected_examples:\n",
        "  print(contrast_exs[\"data\"][idx[0]])\n",
        "  print(\"old predicted\", idx[1])\n",
        "  print(\"new predicted\", idx[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNpdvMFwZb8V",
        "outputId": "2ac145f8-517e-4aca-f64f-1be622273f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'premise': 'A woman with a green headscarf, blue shirt and a very big grin.', 'hypothesis': 'The woman is very emotional.', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'A statue at a museum that no seems to be looking at.', 'hypothesis': 'There is a statue that not many people seem to be know about.', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'A blond-haired doctor and her African american assistant looking threw new medical manuals.', 'hypothesis': 'A african american doctor is looking at a book', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'Male in a blue jacket decides to lay in the grass.', 'hypothesis': 'The guy wearing a blue jacket is laying on the blue grass', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'A woman wearing a ball cap squats down to touch the cracked earth.', 'hypothesis': 'A squatting woman wearing a hat hitting the ground.', 'label': 2}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'Many people standing outside of a place talking to each other in front of a building that has a sign that says \"HI-POINTE.\"', 'hypothesis': 'The group of people are inide of the building.', 'label': 2}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'A group of adults stands in the bathroom looking down a small child in the bath.', 'hypothesis': 'The childs mom is watching her son with his friends.', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'A goofy looking woman is singing on stage.', 'hypothesis': 'A strange woman is singing and dancing on the stage.', 'label': 0}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'people are standing near water with a boat heading their direction', 'hypothesis': 'People are standing near water with a large blue boat heading away from their direction.', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'Four adults eat while sitting on a tile floor.', 'hypothesis': 'Four couples eat food on a kitchen floor.', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'A person in orange clothing rests above a metro entrance.', 'hypothesis': 'A person is waiting for a bus.', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'A skateboarding youth does a trick on a rail.', 'hypothesis': 'Tony Hawk showing his new skateboard tricks to his friends.', 'label': 2}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': 'Girl in a red coat, blue head wrap and jeans is making a snow angel.', 'hypothesis': 'A girl in a red coat makes a snow angel for fun.', 'label': 0}\n",
            "old predicted 1\n",
            "new predicted 1\n",
            "{'premise': \"Island native fishermen reeling in their nets after a long day's work.\", 'hypothesis': 'The men did not go to work today but instead went fishing.', 'label': 0}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'People are on an escalator waiting to get to their destination while looking outside of the glass that makes up the wall.', 'hypothesis': 'People are taking the moving stairs.', 'label': 0}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'A woman is weaving with a comb in her hand.', 'hypothesis': 'A woman is sitting with warm hands.', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'Two men are standing in a boat.', 'hypothesis': 'Some men are standing on top of a boat.', 'label': 0}\n",
            "old predicted 1\n",
            "new predicted 2\n",
            "{'premise': 'A man in a brown jacket stands with his arms spread.', 'hypothesis': 'The man stands on the floor.', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'A black dog with a blue collar is jumping into the water.', 'hypothesis': 'A dog is avoiding the land.', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'Four boys are about to be hit by an approaching wave.', 'hypothesis': 'The wave hits the boys.', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'A lady is helping another woman work in a silver compartment, which is most likely related to nurse-work.', 'hypothesis': 'A lady is working', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'Two people crossing by each other while kite surfing.', 'hypothesis': 'There are many people outdoors.', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n",
            "{'premise': 'A man with no shirt on is performing with a baton.', 'hypothesis': 'A man throws a stick in the air.', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'Female gymnasts warm up before a competition.', 'hypothesis': 'Football players watch.', 'label': 1}\n",
            "old predicted 2\n",
            "new predicted 2\n",
            "{'premise': 'Several women wearing dresses dance in the forest.', 'hypothesis': 'There are people dancing', 'label': 1}\n",
            "old predicted 0\n",
            "new predicted 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_pred_argmax = np.argmax(orig_pred, axis=1)\n",
        "print(orig_pred_argmax)\n",
        "print(orig_labels)\n",
        "cm = confusion_matrix(orig_pred_argmax, orig_labels)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-venjICur-gY",
        "outputId": "e6fadb73-c9c5-4e5d-a5ea-1396c69b3945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 0 1 1 1 2\n",
            " 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "[[36  3  0]\n",
            " [ 1 26  2]\n",
            " [ 3  4 25]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}